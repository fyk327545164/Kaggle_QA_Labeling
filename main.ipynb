import pandas as pd
import numpy as np
%tensorflow_version 2.x
import tensorflow as tf
import tensorflow_hub as hub
from math import floor, ceil
from scipy.stats import spearmanr
import tensorflow.keras.backend as K
from sklearn.model_selection import GroupKFold
from tqdm import tqdm
import sys
sys.path.append('./drive/My Drive/QA/')
import tokenization
import math,  os

print("Tensorflow version " + tf.__version__)

tokenizer = tokenization.FullTokenizer('./drive/My Drive/QA/bert/assets/vocab.txt', True)
output_columns = [
    'question_asker_intent_understanding',
    'question_body_critical',
    'question_conversational',
    'question_expect_short_answer',
    'question_fact_seeking',
    'question_has_commonly_accepted_answer',
    'question_interestingness_others',
    'question_interestingness_self',
    'question_multi_intent',
    'question_not_really_a_question',
    'question_opinion_seeking',
    'question_type_choice',
    'question_type_compare',
    'question_type_consequence',
    'question_type_definition',
    'question_type_entity',
    'question_type_instructions',
    'question_type_procedure',
    'question_type_reason_explanation',
    'question_type_spelling',
    'question_well_written',
    'answer_helpful',
    'answer_level_of_information',
    'answer_plausible',
    'answer_relevance',
    'answer_satisfaction',
    'answer_type_instructions',
    'answer_type_procedure',
    'answer_type_reason_explanation',
    'answer_well_written'
]

df_train = pd.read_csv('./drive/My Drive/QA/data/train.csv')#.iloc[:10,:]
df_test = pd.read_csv('./drive/My Drive/QA/data/test.csv')#.iloc[:10,:]
df_sub = pd.read_csv('./drive/My Drive/QA/data/sample_submission.csv')

%mkdir ../input
%cp -r /content/drive/My\ Drive/QA/bert ../input/bert-model
bert_path = '../input/bert-model'
output_columns = df_sub.columns.values[1:].tolist()

neighbors_dic = [df_train[column].value_counts(normalize=True) for column in output_columns]

def input_ids(tokens, max_length):
    ids = tokenizer.convert_tokens_to_ids(tokens)
    ids = ids + [0] * (max_length - len(ids))
    return ids


def input_mask(tokens, max_length):
    return [1] * len(tokens) + [0] * (max_length - len(tokens))


def segment_ids(tokens, max_length):
    segments = []
    sep = True
    current_id = 0
    for token in tokens:
        segments.append(current_id)
        if token == '[SEP]':
            if sep:
                sep = False
            else:
                current_id = 1
    return segments + [0] * (max_length - len(tokens))


def build_padding(title, body):
    title_max = 29
    body_max = 480

    t = tokenizer.tokenize(title)
    b = tokenizer.tokenize(body)

    title_len = len(t)
    body_len = len(b)

    if (title_len + body_len + 3) > 512:

        if title_max > title_len:
            title_new = title_len
            body_max = body_max + (title_max - title_len)
        else:
            title_new = title_max

        if body_max > body_len:
            body_new = body_len
        else:
            body_new = body_max

        t = t[:title_new]
        b = b[:body_new]

    return t, b


def load_data(df):
    
    question_input_ids, question_input_mask, question_segment_ids = [], [], []
    answer_input_ids, answer_input_mask, answer_segment_ids = [], [], []

    for row in range(df.shape[0]):
        title, body, answer = df.iloc[row, 1], df.iloc[row, 2], df.iloc[row, 5]

        question_t, question_b = build_padding(title, body)
        question_token = ["[CLS]"] + question_t[:] + ["[SEP]"] + question_b + ["[SEP]"]

        answer_t, answer_b = build_padding(title, answer)
        answer_token = ["[CLS]"] + answer_t[:] + ["[SEP]"] + answer_b + ["[SEP]"]

        question_input_ids.append(input_ids(question_token, 512))
        question_input_mask.append(input_mask(question_token, 512))
        question_segment_ids.append(segment_ids(question_token, 512))
        answer_input_ids.append(input_ids(answer_token, 512))
        answer_input_mask.append(input_mask(answer_token, 512))
        answer_segment_ids.append(segment_ids(answer_token, 512))

    return [question_input_ids, question_input_mask, question_segment_ids, answer_input_ids, answer_input_mask,
            answer_segment_ids]


def get_spearmanr(y_true, y_pred):
    rhos = []
    for i, (true, pred) in enumerate(zip(y_true.T, y_pred.T)):
        rho = spearmanr(true, pred).correlation
        if math.isnan(rho):
            rho = -0.01
        rhos.append(rho)
        print("\t" + output_columns[i] + "\t" + str(rho))
    return np.mean(rhos)


class EvalCallback(tf.keras.callbacks.Callback):
    def __init__(self, valid, test, batch_size=16):
        self.valid_inputs = valid[0]
        self.valid_outputs = valid[1]

        self.test_inputs = test
        self.batch_size = batch_size

    def on_train_begin(self, logs=None):
        self.valid_preds = []
        self.test_preds = []

    def on_epoch_end(self, epoch, logs=None):
        pred = self.model.predict(self.valid_inputs, batch_size=self.batch_size)
        self.valid_preds.append(pred)

        rho = get_spearmanr(self.valid_outputs, pred)
        print("Validation rho:" + str(rho))
        rho = get_spearmanr(self.valid_outputs, np.average(self.valid_preds, axis=0))
        print("Validation rho:" + str(rho))

        pred = self.model.predict(self.test_inputs, batch_size=self.batch_size)
        self.test_preds.append(pred)


def question_answer_matching(Q, A):
    W1 = tf.keras.layers.Dense(768, input_shape=(None, None, 768))
    W2 = tf.keras.layers.Dense(768, input_shape=(None, None, 768))
    W3 = tf.keras.layers.Dense(768, input_shape=(None, None, 768))
    W4 = tf.keras.layers.Dense(768, input_shape=(None, 768))
    W5 = tf.keras.layers.Dense(768, input_shape=(None, 768))

    G = tf.keras.activations.softmax(tf.matmul(W1(Q), tf.transpose(A, perm=[0, 2, 1])))
    E_Q = tf.matmul(G, A)
    E_A = tf.matmul(tf.transpose(G, perm=[0, 2, 1]), Q)
    S_Q = tf.keras.activations.relu(W2(E_Q))
    S_A = tf.keras.activations.relu(W3(E_A))
    M_Q = tf.keras.layers.GlobalAveragePooling1D()(S_Q)
    M_A = tf.keras.layers.GlobalAveragePooling1D()(S_A)
    g = tf.keras.activations.sigmoid(W4(M_Q) + W5(M_A))
    M = g * M_Q + (1 - g) * M_A

    return M


def context_match(c1, c2):
    W1 = tf.keras.layers.Dense(768, activation='relu', input_shape=(None, 768))
    W2 = tf.keras.layers.Dense(768, activation='relu', input_shape=(None, 768))
    W3 = tf.keras.layers.Dense(768, input_shape=(None, 768))
    W4 = tf.keras.layers.Dense(768, input_shape=(None, 768))
        
    s1 = W1(c1)
    s2 = W2(c2)

    g = tf.keras.activations.sigmoid(W3(s1) + W4(s2))
    M = g * s1 + (1 - g) * s2
    return M


def build_gate(c1, c2):
    W1 = tf.keras.layers.Dense(768, input_shape=(None, 768))
    W2 = tf.keras.layers.Dense(768, input_shape=(None, 768))
        
    g = tf.keras.activations.sigmoid(W1(c1) + W2(c2))
    M = g * c1 + (1 - g) * c2
    return M
  

def build_model():
    Q_input_word_ids = tf.keras.layers.Input((512,), dtype=tf.int32, name='Q_input_word_ids')
    Q_input_masks = tf.keras.layers.Input((512,), dtype=tf.int32, name='Q_input_masks')
    Q_input_segments_ids = tf.keras.layers.Input((512,), dtype=tf.int32, name='Q_input_segments')

    A_input_word_ids = tf.keras.layers.Input((512,), dtype=tf.int32, name='A_input_word_ids')
    A_input_masks = tf.keras.layers.Input((512,), dtype=tf.int32, name='A_input_masks')
    A_input_segments_ids = tf.keras.layers.Input((512,), dtype=tf.int32, name='A_input_segments')

    Q_bert = hub.KerasLayer(bert_path, trainable=True)
    A_bert = hub.KerasLayer(bert_path, trainable=True)

    _, Q_seq_out = Q_bert([Q_input_word_ids, Q_input_masks, Q_input_segments_ids])
    _, A_seq_out = A_bert([A_input_word_ids, A_input_masks, A_input_segments_ids])

    match_out = question_answer_matching(Q_seq_out, A_seq_out)
    # match_out = tf.keras.layers.Dropout(0.2)(match_out)

    Q_cls_out = tf.reshape(Q_seq_out[:, 0, :], [-1, 768])
    # Q_cls_out = tf.keras.layers.Dropout(0.2)(Q_cls_out)
    A_cls_out = tf.reshape(A_seq_out[:, 0, :], [-1, 768])
    # A_cls_out = tf.keras.layers.Dropout(0.2)(A_cls_out)
    
    cls_match = context_match(Q_cls_out, A_cls_out)
    Q_cls = gate(Q_cls_out, cls_match)
    A_cls = gate(A_cls_out, cls_match)

    Q_out = gate(Q_cls, match_out)
    A_out = gate(A_cls, match_out)

    Q_out = tf.keras.layers.Dropout(0.2)(Q_out)
    Q_out = tf.keras.layers.Dense(21, activation='sigmoid')(Q_out)

    A_out = tf.keras.layers.Dropout(0.2)(A_out)
    A_out = tf.keras.layers.Dense(9, activation='sigmoid')(A_out)

    out = tf.concat([Q_out, A_out], -1)
    
    model = tf.keras.models.Model(inputs=[Q_input_word_ids, Q_input_masks, Q_input_segments_ids,
                                          A_input_word_ids, A_input_masks, A_input_segments_ids], outputs=out)

    return model


def find_nearest_neighbor(d, val):
    cur_w = 1
    cur = 0
    for k, v in d.items():
        w = abs(val - k)
        # w = (1 - v) * (dif**2)
        if w < cur_w:
            cur_w = w
            cur = k
    return cur


def main():
    gfk = GroupKFold(n_splits=6).split(X=df_train.question_body, groups=df_train.question_body)

    inputs = np.asarray(load_data(df_train))
    outputs = np.asarray(df_train[list(df_train.columns[11:])])
    test_inputs = np.asarray(load_data(df_test))
    test_inputs = [test_inputs[i][:] for i in range(6)]
    hists = []

    for fold, (train_ids, valid_ids) in enumerate(gfk):

        if fold < 6:
            tf.keras.backend.clear_session()

            train_inputs = [inputs[i][train_ids] for i in range(6)]
            train_outputs = outputs[train_ids]
            valid_inputs = [inputs[i][valid_ids] for i in range(6)]
            valid_outputs = outputs[valid_ids]

            model = build_model()

            callback = EvalCallback(valid=(valid_inputs, valid_outputs), test=test_inputs)

            op = tf.keras.optimizers.Adam(learning_rate=0.00001)
            model.compile(loss='binary_crossentropy', optimizer=op)
            model.fit(train_inputs, train_outputs, epochs=4, batch_size=1, callbacks=[callback])

            model.save('/content/drive/My Drive/QA/H-fold' + str(fold) + '.h5', include_optimizer=False)

            hists.append(callback.test_preds)
    test_preds = [np.average(hists[i], axis=0) for i in range(len(hists))]
    test_preds = np.average(test_preds, axis=0)

    df_sub.iloc[:, 1:] = test_preds

    
    for i in range(30):
        df_sub.iloc[:,1+i] = [find_nearest_neighbor(neighbors_dic[i], df_sub.iloc[j,1+i]) for j in range(len(df_sub))]

    df_sub.to_csv('/content/drive/My Drive/QA/submission.csv', index=False, float_format="%.2f")

main()
